import io
import os
import json
import re
from typing import Optional, List, Dict

from fastapi import FastAPI, UploadFile, File, HTTPException, Depends
from pydantic import BaseModel
from PIL import Image

import torch
from transformers import AutoProcessor, AutoModelForVision2Seq

from sqlalchemy import create_engine, Column, String, Float, func
from sqlalchemy.orm import sessionmaker, declarative_base, Session

from dotenv import load_dotenv

# .env ë¡œë“œ
load_dotenv()

# SQL ì„¤ì •
DATABASE_URL = os.getenv("FOODY_DATABASE_URL")
if not DATABASE_URL:
    raise RuntimeError("FOODY_DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

engine = create_engine(DATABASE_URL, echo=False, future=True)
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
Base = declarative_base()


def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ê¸°ì¡´ FOOD í…Œì´ë¸” ë§¤í•‘
class Foods(Base):
    __tablename__ = "foods"

    code = Column(String(45), primary_key=True)
    name = Column(String(30), nullable=False)
    standard = Column(String(10), nullable=False)
    kcal = Column(Float, nullable=False, default=0)

    # ì‹¤ì œ DB ì»¬ëŸ¼ ë§¤í•‘
    carb = Column("carb_g", Float, nullable=False, default=0)
    protein = Column("protein_g", Float, nullable=False, default=0)
    fat = Column("fat_g", Float, nullable=False, default=0)
    sugar = Column("sugar_g", Float, nullable=False, default=0)
    natrium = Column("natrium_g", Float, nullable=False, default=0)


# ì‘ë‹µ ëª¨ë¸(JSON)
class FoodResponse(BaseModel):
    name: str
    standard: str
    kcal: float
    carb: float
    protein: float
    fat: float
    sugar: float
    natrium: float


# ìœ í‹¸: ì†Œìˆ˜ì  ë‘˜ì§¸ìë¦¬ ë°˜ì˜¬ë¦¼
def round2(value: float) -> float:
    return round(float(value), 2)


# Qwen VLM í´ë¼ì´ì–¸íŠ¸ (Base model only)
from pathlib import Path

class QwenClient:
    def __init__(self):
        """
        ìš°ì„ ìˆœìœ„:
        1) FOODY_VLM_MODEL_DIR í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •í•œ ë¡œì»¬ ëª¨ë¸/ì²´í¬í¬ì¸íŠ¸
        2) ì—†ìœ¼ë©´ ë² ì´ìŠ¤ ëª¨ë¸ (Qwen/Qwen2.5-VL-3B-Instruct)
        """
        base_id = "Qwen/Qwen2.5-VL-3B-Instruct"
        local_dir = os.getenv("FOODY_VLM_MODEL_DIR", "").strip()

        # 1) ë¡œì»¬ ê²½ë¡œê°€ ì£¼ì–´ì§€ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©
        if local_dir:
            ckpt_path = Path(local_dir)

            # local_dirì´ qwen25_v4 ê°™ì€ ìƒìœ„ í´ë”ë¼ë©´: final > ìµœì‹  checkpoint ìë™ ì„ íƒ
            if ckpt_path.is_dir():
                final_path = ckpt_path / "final"
                if final_path.exists():
                    ckpt_path = final_path
                else:
                    # checkpoint-* ì¤‘ ê°€ì¥ í° step ì„ íƒ
                    checkpoints = sorted(
                        ckpt_path.glob("checkpoint-*"),
                        key=lambda p: int(p.name.split("-")[-1]) if p.name.split("-")[-1].isdigit() else -1
                    )
                    if checkpoints:
                        ckpt_path = checkpoints[-1]

            print(f"[INFO] Loading VLM from local checkpoint: {ckpt_path}")

            # processorëŠ” ë³´í†µ ì²´í¬í¬ì¸íŠ¸ì— ì—†ì„ ìˆ˜ ìˆì–´ì„œ:
            #  - ì²´í¬í¬ì¸íŠ¸ì— ìˆìœ¼ë©´ ê±°ê¸°ì„œ ë¡œë“œ
            #  - ì—†ìœ¼ë©´ ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ
            try:
                self.processor = AutoProcessor.from_pretrained(str(ckpt_path))
                print("[INFO] Processor loaded from checkpoint.")
            except Exception:
                self.processor = AutoProcessor.from_pretrained(base_id)
                print("[WARN] Processor not found in checkpoint. Loaded from base model.")

            # 2) ì²´í¬í¬ì¸íŠ¸ê°€ "í’€ ëª¨ë¸"ì¸ì§€ "LoRA ì–´ëŒ‘í„°"ì¸ì§€ ìë™ íŒë³„
            adapter_cfg = ckpt_path / "adapter_config.json"
            is_lora = adapter_cfg.exists()

            if is_lora:
                print("[INFO] Detected LoRA adapter checkpoint. Loading base + adapter...")
                from peft import PeftModel

                base_model = AutoModelForVision2Seq.from_pretrained(
                    base_id,
                    torch_dtype=torch.float16,
                    device_map="auto",
                )

                self.model = PeftModel.from_pretrained(
                    base_model,
                    str(ckpt_path),
                    torch_dtype=torch.float16,
                    device_map="auto",
                )

                print("[INFO] LoRA adapter loaded successfully.")

            else:
                print("[INFO] Detected full-model checkpoint. Loading directly...")
                self.model = AutoModelForVision2Seq.from_pretrained(
                    str(ckpt_path),
                    torch_dtype=torch.float16,
                    device_map="auto",
                )
                print("[INFO] Full model loaded successfully.")
            

        else:
            # 3) ë¡œì»¬ ì§€ì •ì´ ì—†ìœ¼ë©´ ë² ì´ìŠ¤ ëª¨ë¸
            print("[INFO] Loading base model only (no checkpoint path provided).")
            self.processor = AutoProcessor.from_pretrained(base_id)
            self.model = AutoModelForVision2Seq.from_pretrained(
                base_id,
                torch_dtype=torch.float16,
                device_map="auto",
            )

        self.model.eval()
        print("[INFO] VLM ready.")

    def _is_valid_food_name(self, s: str) -> bool:
        if not s:
            return False
        s = s.strip()

        # í”í•œ ì‹¤íŒ¨ í† í°ë“¤
        if s in {"-", "â€”", "_", "?", "ì—†ìŒ", "ëª¨ë¦„", "ì•Œìˆ˜ì—†ìŒ", "ì•Œ ìˆ˜ ì—†ìŒ", "unknown"}:
            return False

        # í•œê¸€ì´ 1ê¸€ì ì´ìƒì€ ìˆì–´ì•¼ ìŒì‹ëª…ìœ¼ë¡œ ì·¨ê¸‰
        if not re.search(r"[ê°€-í£]", s):
            return False

        return True

    def _generate_one(self, pil_image: Image.Image, prompt: str) -> str:
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": pil_image},
                    {"type": "text", "text": prompt},
                ],
            }
        ]

        inputs = self.processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        ).to(self.model.device)

        output = self.model.generate(
            **inputs,
            max_new_tokens=20,
            do_sample=False,
        )

        text = self.processor.decode(
            output[0][inputs["input_ids"].shape[-1] :],
            skip_special_tokens=True,
        ).strip()

        # ê³µë°±/ë¬¸ì¥ ì„ì—¬ ë‚˜ì˜¬ ë•Œ ëŒ€ë¹„
        text = text.split()[0].strip()

        # ë”°ì˜´í‘œ/ê´„í˜¸ ë“± ì œê±°
        text = re.sub(r"[\"'()\[\]{}<>]", "", text).strip()

        return text

    def _post_process_food_name(self, food_name: str) -> str:
        """ì˜ì–´ ìŒì‹ ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë³€í™˜í•˜ê³  ê²€ì¦"""
        translations = {
            "omelette": "ì˜¤ë¯ˆë ›",
            "omelet": "ì˜¤ë¯ˆë ›",
            "eggroll": "ê³„ë€ë§ì´",
            "egg roll": "ê³„ë€ë§ì´",
            "koreaneggroll": "ê³„ë€ë§ì´",
            "korean egg roll": "ê³„ë€ë§ì´",
            "rolledegg": "ê³„ë€ë§ì´",
            "rolled egg": "ê³„ë€ë§ì´",
            "salad": "ìƒëŸ¬ë“œ",
            "rice": "ë°¥",
            "kimchi": "ê¹€ì¹˜",
            "kimbap": "ê¹€ë°¥",
            "ramen": "ë¼ë©´",
        }

        lower_name = food_name.lower().strip()

        # ì˜ì–´ ë‹¨ì–´ ê·¸ëŒ€ë¡œ ë§¤ì¹­
        if lower_name in translations:
            return translations[lower_name]

        # ë¶€ë¶„ ì¼ì¹˜ ì²˜ë¦¬
        for eng, kor in translations.items():
            if eng in lower_name:
                return kor

        return food_name

    # 1) ìŒì‹ ì´ë¯¸ì§€ë¥¼ ë³´ê³  ìŒì‹ ëª… ì¶”ë¡ 
    def predict_food_name(self, pil_image: Image.Image) -> str:
        prompt1 = (
            "ë„ˆëŠ” í•œêµ­ ìŒì‹ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°ë‹¤.\n"
            "ì´ë¯¸ì§€ì—ì„œ 'ê°€ì¥ ì¤‘ì‹¬ì´ ë˜ëŠ” ìŒì‹ 1ê°œ'ì˜ ì´ë¦„ë§Œ í•œêµ­ì–´ë¡œ ì¶œë ¥í•´ë¼.\n\n"
            "ì¶œë ¥ ê·œì¹™(ë§¤ìš° ì¤‘ìš”):\n"
            "1) í•œêµ­ì–´ ìŒì‹ëª…ë§Œ ì¶œë ¥\n"
            "2) ì¡°ì‚¬/ë¬¸ì¥/ì„¤ëª… ê¸ˆì§€ (ì˜ˆ: 'ì…ë‹ˆë‹¤', 'ê°™ì•„ìš”', '.' ê¸ˆì§€)\n"
            "3) ë”°ì˜´í‘œ/ê´„í˜¸/ìŠ¬ë˜ì‹œ/ì´ëª¨ì§€ ê¸ˆì§€\n"
            "4) ê³µë°± ì—†ì´ ìŒì‹ëª…ë§Œ ì¶œë ¥ (ìµœëŒ€ 12ì)\n\n"
            "ì˜ˆì‹œ(ì •ë‹µ í˜•ì‹):\n"
            "ê¹€ë°¥\n"
            "ê³„ë€ë§ì´\n"
            "ë–¡ë³¶ì´\n\n"
            "í˜¼ë™ ì£¼ì˜ ê·œì¹™:\n"
            "- ê³„ë€ë§ì´: ë‹¬ê±€ì„ ë§ì•„ ë„¤ëª¨/ì›í†µ í˜•íƒœ, ë‹¨ë©´ì´ ë§ë¦° ì¸µ\n"
            "- ì˜¤ë¯ˆë ›: ì ‘íŒ í˜•íƒœ, ë‘¥ê¸€ê³  ë‚©ì‘í•¨\n\n"
            "ì¤‘ìš”: ì¶œë ¥í•  ìŒì‹ëª…ì´ ì—†ë‹¤ê³  íŒë‹¨ë˜ë”ë¼ë„ '-', 'ì—†ìŒ', '?'ë¥¼ ì¶œë ¥í•˜ì§€ ë§ê³ \n"
            "ê°€ì¥ ìœ ì‚¬í•œ í•œêµ­ ìŒì‹ëª… 1ê°œë¥¼ ë°˜ë“œì‹œ ì¶œë ¥í•˜ë¼.\n\n"
            "ì •ë‹µ(ìŒì‹ëª…ë§Œ):"
        )

        out1 = self._generate_one(pil_image, prompt1)
        out1 = self._post_process_food_name(out1)
        if self._is_valid_food_name(out1):
            return out1

        # 2ì°¨ ì‹œë„ í”„ë¡¬í”„íŠ¸ (ë” ê°„ë‹¨íˆ)
        prompt2 = (
            "ë„ˆëŠ” í•œêµ­ ìŒì‹ ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°ë‹¤.\n"
            "ëª¨ë¥´ê² ì–´ë„ '-', '?', 'ì—†ìŒ'ì„ ì¶œë ¥í•˜ì§€ ë§ˆë¼.\n"
            "ê°€ì¥ ìœ ì‚¬í•œ í•œêµ­ ìŒì‹ëª… 1ê°œë¥¼ ë°˜ë“œì‹œ ì¶œë ¥í•˜ë¼.\n"
            "ìŒì‹ëª…ë§Œ ì¶œë ¥(ì„¤ëª… ê¸ˆì§€).\n"
            "ì •ë‹µ:"
        )

        out2 = self._generate_one(pil_image, prompt2)
        out2 = self._post_process_food_name(out2)
        if self._is_valid_food_name(out2):
            return out2

        # ìµœí›„ fallback: ì˜ˆì „ì²˜ëŸ¼ ì´ìƒê°’ ë…¸ì¶œì„ ë§‰ê¸° ìœ„í•œ ê¸°ë³¸ê°’
        print(f"[WARN] VLM invalid outputs: out1='{out1}', out2='{out2}' -> fallback='ìŒì‹'")
        return "ìŒì‹"

    # 2) (Fallback) ìˆœìˆ˜ LLM ê¸°ë°˜ ì˜ì–‘ ì¶”ë¡  (RAG ì‹¤íŒ¨ ì‹œ ë§ˆì§€ë§‰ ë³´ë£¨)
    def estimate_nutrition_llm(self, food_name: str) -> dict:
        prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ì˜ì–‘í•™ìì…ë‹ˆë‹¤.
"{food_name}" ìŒì‹ì˜ 100g ê¸°ì¤€ ì˜ì–‘ì„±ë¶„ì„ ì¶”ì •í•˜ì„¸ìš”.

 ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
 ì„¤ëª…, ë¬¸ì¥, ì½”ë“œë¸”ë¡, ì—¬ë¶„ì˜ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ë¡œ ë„£ì§€ ë§ˆì„¸ìš”.
 ëª¨ë“  ìˆ˜ì¹˜ëŠ” number ë¡œ ì¶œë ¥í•˜ì„¸ìš” (ë”°ì˜´í‘œ ê¸ˆì§€)

ì˜ˆì‹œ ì¶œë ¥:
{{
  "standard": "100g",
  "kcal": 154,
  "carb": 3.2,
  "protein": 11.2,
  "fat": 10.1,
  "sugar": 1.1,
  "natrium": 250
}}
        """

        messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]

        inputs = self.processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        ).to(self.model.device)

        output = self.model.generate(**inputs, max_new_tokens=200)

        text = self.processor.decode(
            output[0][inputs["input_ids"].shape[-1] :],
            skip_special_tokens=True,
        ).strip()

        try:
            json_block = re.search(r"\{.*\}", text, flags=re.S).group(0)
            data = json.loads(json_block)
        except Exception:
            data = {
                "standard": "100g",
                "kcal": 0,
                "carb": 0,
                "protein": 0,
                "fat": 0,
                "sugar": 0,
                "natrium": 0,
            }

        return data


qwen = QwenClient()

# Chroma RAG ì„¤ì •
import chromadb
from chromadb.utils import embedding_functions

CHROMA_DB_DIR = os.getenv("FOODY_CHROMA_DIR", "./chroma_foods")
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)

ko_embedding = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=os.getenv("FOODY_EMBED_MODEL", "jhgan/ko-sroberta-multitask")
)

food_collection = chroma_client.get_or_create_collection(
    name="food_nutrition",
    embedding_function=ko_embedding,
)


def build_chroma_from_db():
    """ì„œë²„ ì‹œì‘ ì‹œ foods í…Œì´ë¸” ë‚´ìš©ì„ Chromaì— ì¸ë±ì‹± (ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)"""
    count = food_collection.count()
    if count > 0:
        print(f"[INFO] Existing Chroma collection already has {count} items. Skip building.")
        return

    print("[INFO] Building Chroma index from foods table...")

    db = SessionLocal()
    try:
        foods: List[Foods] = db.query(Foods).all()
        total = len(foods)
        print(f"[INFO] Loaded {total} foods from DB.")
    finally:
        try:
            db.close()
        except Exception as e:
            print(f"[WARN] Failed to close DB session cleanly: {e}")

    if total == 0:
        print("[INFO] No foods found in DB to index.")
        return

    batch_size = 128
    for start in range(0, total, batch_size):
        end = min(start + batch_size, total)
        batch = foods[start:end]

        ids = []
        docs = []
        metas = []

        for f in batch:
            ids.append(f.code)
            docs.append(f"{f.name} {f.standard}")
            metas.append(
                {
                    "name": f.name,
                    "standard": f.standard,
                    "kcal": float(f.kcal),
                    "carb": float(f.carb),
                    "protein": float(f.protein),
                    "fat": float(f.fat),
                    "sugar": float(f.sugar),
                    "natrium": float(f.natrium),
                }
            )

        food_collection.add(ids=ids, documents=docs, metadatas=metas)
        print(f"[INFO] Indexed {end}/{total} foods into Chroma...")

    print("[INFO] Finished building Chroma index.")


def rag_estimate_nutrition(
    food_name: str,
    top_k: int = 3,
    hard_threshold: float = 1.0,      # 1ë²ˆ: ì´ê²ƒ ë„˜ìœ¼ë©´ RAG ìì²´ë¥¼ ë²„ë¦¼
    soft_threshold: float = 0.75,     # (ì„ íƒ) 2ë²ˆ: ì´ ì•ˆìª½ í›„ë³´ë“¤ë§Œ í‰ê· ì— ì°¸ì—¬
    eps: float = 1e-6
) -> Optional[dict]:
    """
    - Top-K ê²€ìƒ‰
    - best_distance > hard_threshold ì´ë©´ None (LLM í´ë°± ìœ ë„)
    - ì•„ë‹ˆë©´ (dist <= soft_threshold) í›„ë³´ë§Œ ê³¨ë¼ inverse-distance ê°€ì¤‘í‰ê· 
      (ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í›„ë³´ê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ Top-1 ì‚¬ìš©)
    """
    result = food_collection.query(query_texts=[food_name], n_results=top_k)
    metadatas = result.get("metadatas", [[]])[0]
    distances = result.get("distances", [[]])[0]
    documents = result.get("documents", [[]])[0]

    if not metadatas:
        print(f"[WARN] No RAG results found for '{food_name}'")
        return None

    # ë¡œê·¸
    print(f"[DEBUG] RAG Search Results for '{food_name}':")
    for i, (doc, meta, dist) in enumerate(zip(documents, metadatas, distances)):
        print(f"  [{i+1}] {doc} | distance={dist:.4f}")

    best_distance = distances[0]
    best_meta = metadatas[0]

    # âœ… 1ë²ˆ: hard threshold ë„˜ìœ¼ë©´ RAG íê¸°
    if best_distance > hard_threshold:
        print(f"[WARN] Best match distance ({best_distance:.4f}) > hard_threshold ({hard_threshold}).")
        print("[WARN] Discarding RAG result -> fallback to LLM.")
        return None

    # âœ… 2ë²ˆ: soft_threshold ì•ˆìª½ë§Œ í‰ê· ì— ì°¸ì—¬ (ì—†ìœ¼ë©´ Top-1)
    candidates = []
    for meta, dist in zip(metadatas, distances):
        if dist <= soft_threshold:
            candidates.append((meta, dist))

    # í›„ë³´ê°€ ë„ˆë¬´ ì—†ìœ¼ë©´ Top-1ë§Œ ì‚¬ìš©
    if not candidates:
        print(f"[INFO] No candidates within soft_threshold ({soft_threshold}). Using Top-1 only.")
        return {
            "standard": best_meta.get("standard", "100g"),
            "kcal": float(best_meta.get("kcal", 0)),
            "carb": float(best_meta.get("carb", 0)),
            "protein": float(best_meta.get("protein", 0)),
            "fat": float(best_meta.get("fat", 0)),
            "sugar": float(best_meta.get("sugar", 0)),
            "natrium": float(best_meta.get("natrium", 0)),
        }

    # inverse-distance ê°€ì¤‘ì¹˜ (ê°€ê¹Œìš¸ìˆ˜ë¡ weight í¼)
    weights = [1.0 / (d + eps) for _, d in candidates]
    wsum = sum(weights)

    def wavg(key: str, default=0.0) -> float:
        s = 0.0
        for (meta, _), w in zip(candidates, weights):
            s += float(meta.get(key, default)) * w
        return s / wsum if wsum > 0 else float(best_meta.get(key, default))

    # standardëŠ” ìˆ«ì í‰ê· ë³´ë‹¤ "best_meta"ë¥¼ ë”°ë¥´ëŠ” ê²Œ ë³´í†µ ì•ˆì „
    result_data = {
        "standard": best_meta.get("standard", "100g"),
        "kcal": wavg("kcal", 0),
        "carb": wavg("carb", 0),
        "protein": wavg("protein", 0),
        "fat": wavg("fat", 0),
        "sugar": wavg("sugar", 0),
        "natrium": wavg("natrium", 0),
    }

    print(
        f"[INFO] Weighted RAG selected (best_distance={best_distance:.4f}, "
        f"used_candidates={len(candidates)}/{len(metadatas)})"
    )
    return result_data



# FastAPI ì•± ìƒì„±
app = FastAPI(title="Foody - Qwen2.5-VL Analyzer API")


@app.on_event("startup")
def on_startup():
    build_chroma_from_db()


@app.post("/api/vlm/food", response_model=FoodResponse)
async def predict_food(
    image: UploadFile = File(...),
    db: Session = Depends(get_db),
):
    # 1) ì´ë¯¸ì§€ ì²´í¬
    if not image.content_type or not image.content_type.startswith("image/"):
        raise HTTPException(400, "ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

    # 2) ì´ë¯¸ì§€ ë¡œë“œ
    try:
        content = await image.read()
        pil_image = Image.open(io.BytesIO(content)).convert("RGB")
    except Exception:
        raise HTTPException(400, "ì´ë¯¸ì§€ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 3) ìŒì‹ ì´ë¦„ ì¶”ë¡  (VLM)
    print(f"\n{'='*80}")
    food_name = qwen.predict_food_name(pil_image)
    normalized_name = food_name.replace(" ", "")
    print(f"[INFO] ğŸ” VLM Prediction: '{food_name}' (normalized: '{normalized_name}')")

    # 4) RDB exact match (ê³µë°± ì œê±° ë¹„êµ)
    food = (
        db.query(Foods)
        .filter(func.replace(Foods.name, " ", "") == normalized_name)
        .first()
    )

    # 5) ì˜ì–‘ ì„±ë¶„ ê²°ì • ë¡œì§
    if food:
        # âœ… DBì— ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ìŒì‹ì´ ìˆìœ¼ë©´ DB ë°ì´í„° ì‚¬ìš©
        print(f"[INFO] Found exact match in DB: '{food.name}' (code: {food.code})")
        print(f"[INFO] Using DB nutrition data directly (skipping RAG)")
        
        response_name = food.name
        est = {
            "standard": food.standard,
            "kcal": float(food.kcal),
            "carb": float(food.carb),
            "protein": float(food.protein),
            "fat": float(food.fat),
            "sugar": float(food.sugar),
            "natrium": float(food.natrium),
        }
    else:
        # DBì— ì—†ìœ¼ë©´ RAGë¡œ ìœ ì‚¬ ìŒì‹ ê²€ìƒ‰
        print(f"[INFO] No exact match in DB for '{normalized_name}'")
        print(f"[INFO] Starting RAG search for '{food_name}'...")
        
        response_name = food_name
        est = rag_estimate_nutrition(food_name)
        
        if est:
            print(f"[INFO] RAG search successful")
        else:
            # RAGë„ ì‹¤íŒ¨í•˜ë©´ LLMìœ¼ë¡œ ì¶”ì •
            print(f"[INFO] RAG failed, using LLM estimation for '{food_name}'")
            est = qwen.estimate_nutrition_llm(food_name)

    # ìµœì¢… ê²°ê³¼ ë¡œê¹…
    print(f"[INFO] Final Response:")
    print(f"name='{response_name}', standard='{est.get('standard')}'")
    print(f"kcal={round2(est.get('kcal', 0))}, carb={round2(est.get('carb', 0))}g")
    print(f"protein={round2(est.get('protein', 0))}g, fat={round2(est.get('fat', 0))}g")
    print(f"sugar={round2(est.get('sugar', 0))}g, natrium={round2(est.get('natrium', 0))}mg")
    print(f"{'='*80}\n")

    return FoodResponse(
        name=response_name,
        standard=est.get("standard", "100"),
        kcal=round2(est.get("kcal", 0)),
        carb=round2(est.get("carb", 0)),
        protein=round2(est.get("protein", 0)),
        fat=round2(est.get("fat", 0)),
        sugar=round2(est.get("sugar", 0)),
        natrium=round2(est.get("natrium", 0)),
    )

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "app:app",
        host="0.0.0.0", 
        port=8000,    
        reload=False      
    )

