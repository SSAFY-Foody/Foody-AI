import io
import os
import json
import re
from typing import Optional, List, Dict, Tuple

from fastapi import FastAPI, UploadFile, File, HTTPException, Depends
from pydantic import BaseModel
from PIL import Image

import torch
from transformers import AutoProcessor, AutoModelForVision2Seq

from sqlalchemy import create_engine, Column, String, Float, func
from sqlalchemy.orm import sessionmaker, declarative_base, Session

from dotenv import load_dotenv

# .env Î°úÎìú
load_dotenv()

# SQL ÏÑ§Ï†ï
DATABASE_URL = os.getenv("FOODY_DATABASE_URL")
if not DATABASE_URL:
    raise RuntimeError("FOODY_DATABASE_URL ÌôòÍ≤ΩÎ≥ÄÏàòÍ∞Ä ÏÑ§Ï†ïÎêòÏñ¥ ÏûàÏßÄ ÏïäÏäµÎãàÎã§.")

engine = create_engine(DATABASE_URL, echo=False, future=True)
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)
Base = declarative_base()


def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# Í∏∞Ï°¥ FOOD ÌÖåÏù¥Î∏î Îß§Ìïë
class Foods(Base):
    __tablename__ = "foods"

    code = Column(String(45), primary_key=True)
    name = Column(String(30), nullable=False)
    standard = Column(String(10), nullable=False)
    kcal = Column(Float, nullable=False, default=0)

    # Ïã§Ï†ú DB Ïª¨Îüº Îß§Ìïë
    carb = Column("carb_g", Float, nullable=False, default=0)
    protein = Column("protein_g", Float, nullable=False, default=0)
    fat = Column("fat_g", Float, nullable=False, default=0)
    sugar = Column("sugar_g", Float, nullable=False, default=0)
    natrium = Column("natrium_g", Float, nullable=False, default=0)


# ÏùëÎãµ Î™®Îç∏(JSON)
class FoodResponse(BaseModel):
    name: str
    standard: str
    kcal: float
    carb: float
    protein: float
    fat: float
    sugar: float
    natrium: float


def round2(value: float) -> float:
    return round(float(value), 2)


# Qwen VLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏
from pathlib import Path

class QwenClient:
    def __init__(self):
        base_id = "Qwen/Qwen2.5-VL-3B-Instruct"
        local_dir = os.getenv("FOODY_VLM_MODEL_DIR", "").strip()

        if local_dir:
            ckpt_path = Path(local_dir)

            if ckpt_path.is_dir():
                final_path = ckpt_path / "final"
                if final_path.exists():
                    ckpt_path = final_path
                else:
                    checkpoints = sorted(
                        ckpt_path.glob("checkpoint-*"),
                        key=lambda p: int(p.name.split("-")[-1]) if p.name.split("-")[-1].isdigit() else -1
                    )
                    if checkpoints:
                        ckpt_path = checkpoints[-1]

            print(f"[INFO] Loading VLM from local checkpoint: {ckpt_path}")

            try:
                self.processor = AutoProcessor.from_pretrained(str(ckpt_path))
                print("[INFO] Processor loaded from checkpoint.")
            except Exception:
                self.processor = AutoProcessor.from_pretrained(base_id)
                print("[WARN] Processor not found in checkpoint. Loaded from base model.")

            adapter_cfg = ckpt_path / "adapter_config.json"
            is_lora = adapter_cfg.exists()

            if is_lora:
                print("[INFO] Detected LoRA adapter checkpoint. Loading base + adapter...")
                from peft import PeftModel

                base_model = AutoModelForVision2Seq.from_pretrained(
                    base_id,
                    torch_dtype=torch.float16,
                    device_map="auto",
                )

                self.model = PeftModel.from_pretrained(
                    base_model,
                    str(ckpt_path),
                    torch_dtype=torch.float16,
                    device_map="auto",
                )
                print("[INFO] LoRA adapter loaded successfully.")
            else:
                print("[INFO] Detected full-model checkpoint. Loading directly...")
                self.model = AutoModelForVision2Seq.from_pretrained(
                    str(ckpt_path),
                    torch_dtype=torch.float16,
                    device_map="auto",
                )
                print("[INFO] Full model loaded successfully.")
        else:
            print("[INFO] Loading base model only (no checkpoint path provided).")
            self.processor = AutoProcessor.from_pretrained(base_id)
            self.model = AutoModelForVision2Seq.from_pretrained(
                base_id,
                torch_dtype=torch.float16,
                device_map="auto",
            )

        self.model.eval()
        print("[INFO] VLM ready.")

    def _is_valid_food_name(self, s: str) -> bool:
        if not s:
            return False
        s = s.strip()
        if s in {"-", "‚Äî", "_", "?", "ÏóÜÏùå", "Î™®Î¶Ñ", "ÏïåÏàòÏóÜÏùå", "Ïïå Ïàò ÏóÜÏùå", "unknown"}:
            return False
        if not re.search(r"[Í∞Ä-Ìû£]", s):
            return False
        return True

    def _generate_one(self, pil_image: Image.Image, prompt: str) -> str:
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": pil_image},
                    {"type": "text", "text": prompt},
                ],
            }
        ]

        inputs = self.processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        ).to(self.model.device)

        output = self.model.generate(
            **inputs,
            max_new_tokens=20,
            do_sample=False,
        )

        text = self.processor.decode(
            output[0][inputs["input_ids"].shape[-1] :],
            skip_special_tokens=True,
        ).strip()

        text = text.split()[0].strip()
        text = re.sub(r"[\"'()\[\]{}<>]", "", text).strip()
        return text

    def _post_process_food_name(self, food_name: str) -> str:
        translations = {
            "omelette": "Ïò§ÎØàÎ†õ",
            "omelet": "Ïò§ÎØàÎ†õ",
            "eggroll": "Í≥ÑÎûÄÎßêÏù¥",
            "egg roll": "Í≥ÑÎûÄÎßêÏù¥",
            "koreaneggroll": "Í≥ÑÎûÄÎßêÏù¥",
            "korean egg roll": "Í≥ÑÎûÄÎßêÏù¥",
            "rolledegg": "Í≥ÑÎûÄÎßêÏù¥",
            "rolled egg": "Í≥ÑÎûÄÎßêÏù¥",
            "salad": "ÏÉêÎü¨Îìú",
            "rice": "Î∞•",
            "kimchi": "ÍπÄÏπò",
            "kimbap": "ÍπÄÎ∞•",
            "ramen": "ÎùºÎ©¥",
        }

        lower_name = food_name.lower().strip()
        if lower_name in translations:
            return translations[lower_name]
        for eng, kor in translations.items():
            if eng in lower_name:
                return kor
        return food_name

    def predict_food_name(self, pil_image: Image.Image) -> str:
        prompt1 = (
            "ÎÑàÎäî ÌïúÍµ≠ ÏùåÏãù Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•òÍ∏∞Îã§.\n"
            "Ïù¥ÎØ∏ÏßÄÏóêÏÑú 'Í∞ÄÏû• Ï§ëÏã¨Ïù¥ ÎêòÎäî ÏùåÏãù 1Í∞ú'Ïùò Ïù¥Î¶ÑÎßå ÌïúÍµ≠Ïñ¥Î°ú Ï∂úÎ†•Ìï¥Îùº.\n\n"
            "Ï∂úÎ†• Í∑úÏπô(Îß§Ïö∞ Ï§ëÏöî):\n"
            "1) ÌïúÍµ≠Ïñ¥ ÏùåÏãùÎ™ÖÎßå Ï∂úÎ†•\n"
            "2) Ï°∞ÏÇ¨/Î¨∏Ïû•/ÏÑ§Î™Ö Í∏àÏßÄ (Ïòà: 'ÏûÖÎãàÎã§', 'Í∞ôÏïÑÏöî', '.' Í∏àÏßÄ)\n"
            "3) Îî∞Ïò¥Ìëú/Í¥ÑÌò∏/Ïä¨ÎûòÏãú/Ïù¥Î™®ÏßÄ Í∏àÏßÄ\n"
            "4) Í≥µÎ∞± ÏóÜÏù¥ ÏùåÏãùÎ™ÖÎßå Ï∂úÎ†• (ÏµúÎåÄ 12Ïûê)\n\n"
            "ÏòàÏãú(Ï†ïÎãµ ÌòïÏãù):\n"
            "ÍπÄÎ∞•\n"
            "Í≥ÑÎûÄÎßêÏù¥\n"
            "Îñ°Î≥∂Ïù¥\n\n"
            "ÌòºÎèô Ï£ºÏùò Í∑úÏπô:\n"
            "- Í≥ÑÎûÄÎßêÏù¥: Îã¨Í±ÄÏùÑ ÎßêÏïÑ ÎÑ§Î™®/ÏõêÌÜµ ÌòïÌÉú, Îã®Î©¥Ïù¥ ÎßêÎ¶∞ Ï∏µ\n"
            "- Ïò§ÎØàÎ†õ: Ï†ëÌûå ÌòïÌÉú, Îë•Í∏ÄÍ≥† ÎÇ©ÏûëÌï®\n\n"
            "Ï§ëÏöî: Ï∂úÎ†•Ìï† ÏùåÏãùÎ™ÖÏù¥ ÏóÜÎã§Í≥† ÌåêÎã®ÎêòÎçîÎùºÎèÑ '-', 'ÏóÜÏùå', '?'Î•º Ï∂úÎ†•ÌïòÏßÄ ÎßêÍ≥†\n"
            "Í∞ÄÏû• Ïú†ÏÇ¨Ìïú ÌïúÍµ≠ ÏùåÏãùÎ™Ö 1Í∞úÎ•º Î∞òÎìúÏãú Ï∂úÎ†•ÌïòÎùº.\n\n"
            "Ï†ïÎãµ(ÏùåÏãùÎ™ÖÎßå):"
        )

        out1 = self._generate_one(pil_image, prompt1)
        out1 = self._post_process_food_name(out1)
        if self._is_valid_food_name(out1):
            return out1

        prompt2 = (
            "ÎÑàÎäî ÌïúÍµ≠ ÏùåÏãù Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•òÍ∏∞Îã§.\n"
            "Î™®Î•¥Í≤†Ïñ¥ÎèÑ '-', '?', 'ÏóÜÏùå'ÏùÑ Ï∂úÎ†•ÌïòÏßÄ ÎßàÎùº.\n"
            "Í∞ÄÏû• Ïú†ÏÇ¨Ìïú ÌïúÍµ≠ ÏùåÏãùÎ™Ö 1Í∞úÎ•º Î∞òÎìúÏãú Ï∂úÎ†•ÌïòÎùº.\n"
            "ÏùåÏãùÎ™ÖÎßå Ï∂úÎ†•(ÏÑ§Î™Ö Í∏àÏßÄ).\n"
            "Ï†ïÎãµ:"
        )

        out2 = self._generate_one(pil_image, prompt2)
        out2 = self._post_process_food_name(out2)
        if self._is_valid_food_name(out2):
            return out2

        print(f"[WARN] VLM invalid outputs: out1='{out1}', out2='{out2}' -> fallback='ÏùåÏãù'")
        return "ÏùåÏãù"

    def estimate_nutrition_llm(self, food_name: str) -> dict:
        prompt = f"""
ÎãπÏã†ÏùÄ Ï†ÑÎ¨∏ ÏòÅÏñëÌïôÏûêÏûÖÎãàÎã§.
"{food_name}" ÏùåÏãùÏùò 100g Í∏∞Ï§Ä ÏòÅÏñëÏÑ±Î∂ÑÏùÑ Ï∂îÏ†ïÌïòÏÑ∏Ïöî.

 Î∞òÎìúÏãú ÏïÑÎûò JSON ÌòïÏãùÏúºÎ°úÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî.
 ÏÑ§Î™Ö, Î¨∏Ïû•, ÏΩîÎìúÎ∏îÎ°ù, Ïó¨Î∂ÑÏùò ÌÖçÏä§Ìä∏Îäî Ï†àÎåÄÎ°ú ÎÑ£ÏßÄ ÎßàÏÑ∏Ïöî.
 Î™®Îì† ÏàòÏπòÎäî number Î°ú Ï∂úÎ†•ÌïòÏÑ∏Ïöî (Îî∞Ïò¥Ìëú Í∏àÏßÄ)

ÏòàÏãú Ï∂úÎ†•:
{{
  "standard": "100g",
  "kcal": 154,
  "carb": 3.2,
  "protein": 11.2,
  "fat": 10.1,
  "sugar": 1.1,
  "natrium": 250
}}
        """

        messages = [{"role": "user", "content": [{"type": "text", "text": prompt}]}]

        inputs = self.processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        ).to(self.model.device)

        output = self.model.generate(**inputs, max_new_tokens=200)

        text = self.processor.decode(
            output[0][inputs["input_ids"].shape[-1] :],
            skip_special_tokens=True,
        ).strip()

        try:
            json_block = re.search(r"\{.*\}", text, flags=re.S).group(0)
            data = json.loads(json_block)
        except Exception:
            data = {
                "standard": "100g",
                "kcal": 0,
                "carb": 0,
                "protein": 0,
                "fat": 0,
                "sugar": 0,
                "natrium": 0,
            }

        return data


qwen = QwenClient()

# Chroma RAG
# - ChromaÏóêÎäî "Í≤ÄÏÉâÏö© ÌÖçÏä§Ìä∏ + code"Îßå Ï†ÄÏû•
# - ÏòÅÏñëÍ∞íÏùÄ Ìï≠ÏÉÅ RDBÏóêÏÑú Í∞ÄÏ†∏Ïò¥ (source of truth)
import chromadb
from chromadb.utils import embedding_functions

CHROMA_DB_DIR = os.getenv("FOODY_CHROMA_DIR", "./chroma_foods")
chroma_client = chromadb.PersistentClient(path=CHROMA_DB_DIR)

ko_embedding = embedding_functions.SentenceTransformerEmbeddingFunction(
    model_name=os.getenv("FOODY_EMBED_MODEL", "jhgan/ko-sroberta-multitask")
)

food_collection = chroma_client.get_or_create_collection(
    name="food_nutrition",
    embedding_function=ko_embedding,
)


def build_chroma_from_db():
    """
    ÏÑúÎ≤Ñ ÏãúÏûë Ïãú foods ÌÖåÏù¥Î∏î ÎÇ¥Ïö©ÏùÑ ChromaÏóê Ïù∏Îç±Ïã±.
    ‚úÖ ChromaÏóêÎäî Ï§ëÎ≥µ ÏòÅÏñëÍ∞í Ï†ÄÏû• X (code + name/standard Ï†ïÎèÑÎßå)
    """
    count = food_collection.count()
    if count > 0:
        print(f"[INFO] Existing Chroma collection already has {count} items. Skip building.")
        return

    print("[INFO] Building Chroma index from foods table (lightweight metadata)...")

    db = SessionLocal()
    try:
        foods: List[Foods] = db.query(Foods.code, Foods.name, Foods.standard).all()
        total = len(foods)
        print(f"[INFO] Loaded {total} foods from DB.")
    finally:
        try:
            db.close()
        except Exception as e:
            print(f"[WARN] Failed to close DB session cleanly: {e}")

    if total == 0:
        print("[INFO] No foods found in DB to index.")
        return

    batch_size = 256
    for start in range(0, total, batch_size):
        end = min(start + batch_size, total)
        batch = foods[start:end]

        ids, docs, metas = [], [], []
        for code, name, standard in batch:
            ids.append(code)
            # Í≤ÄÏÉâÏö© Î¨∏ÏÑú(Ïù¥Î¶Ñ Í∏∞Î∞ò Ïú†ÏÇ¨Í≤ÄÏÉâ) + Îã®ÏúÑ(ÏÑ†ÌÉù)
            docs.append(f"{name} {standard}")
            metas.append({"code": code, "name": name, "standard": standard})

        food_collection.add(ids=ids, documents=docs, metadatas=metas)
        print(f"[INFO] Indexed {end}/{total} foods into Chroma...")

    print("[INFO] Finished building Chroma index.")


def _fetch_foods_by_codes(db: Session, codes: List[str]) -> Dict[str, Foods]:
    """codes Î™©Î°ùÏùÑ RDBÏóêÏÑú Ï°∞ÌöåÌï¥ÏÑú {code: Foods}Î°ú Î∞òÌôò"""
    if not codes:
        return {}
    rows = db.query(Foods).filter(Foods.code.in_(codes)).all()
    return {r.code: r for r in rows}


def rag_pick_codes(
    query_text: str,
    top_k: int = 5,
    hard_threshold: float = 1.0,
    soft_threshold: float = 0.75,
) -> Optional[List[Tuple[str, float]]]:
    """
    ChromaÏóêÏÑú Ïú†ÏÇ¨ Í≤ÄÏÉâ Í≤∞Í≥ºÎ•º code + distanceÎ°ú Î∞òÌôò.
    - best_distance > hard_threshold -> None (RAG ÌèêÍ∏∞)
    - dist <= soft_threshold ÌõÑÎ≥¥Îì§Îßå Î¶¨ÌÑ¥ (ÏóÜÏúºÎ©¥ Top-1Îßå)
    """
    result = food_collection.query(query_texts=[query_text], n_results=top_k)
    metadatas = result.get("metadatas", [[]])[0]
    distances = result.get("distances", [[]])[0]
    documents = result.get("documents", [[]])[0]

    if not metadatas:
        print(f"[WARN] No RAG results found for '{query_text}'")
        return None

    print(f"[DEBUG] RAG Search Results for '{query_text}':")
    for i, (doc, meta, dist) in enumerate(zip(documents, metadatas, distances)):
        print(f"  [{i+1}] {doc} | code={meta.get('code')} | distance={dist:.4f}")

    best_distance = distances[0]
    if best_distance > hard_threshold:
        print(f"[WARN] Best match distance ({best_distance:.4f}) > hard_threshold ({hard_threshold}).")
        print("[WARN] Discarding RAG result -> fallback to LLM.")
        return None

    candidates: List[Tuple[str, float]] = []
    for meta, dist in zip(metadatas, distances):
        code = meta.get("code")
        if not code:
            continue
        if dist <= soft_threshold:
            candidates.append((code, float(dist)))

    if not candidates:
        # soft_threshold ÏïàÏ™Ω ÌõÑÎ≥¥Í∞Ä ÏóÜÏúºÎ©¥ Top-1 codeÎßå
        top1_code = metadatas[0].get("code")
        if not top1_code:
            return None
        print(f"[INFO] No candidates within soft_threshold ({soft_threshold}). Using Top-1 only.")
        return [(top1_code, float(distances[0]))]

    print(f"[INFO] Candidate codes within soft_threshold: {len(candidates)}")
    return candidates


def rag_estimate_nutrition_from_rdb(
    db: Session,
    food_name: str,
    top_k: int = 5,
    hard_threshold: float = 1.0,
    soft_threshold: float = 0.75,
    eps: float = 1e-6
) -> Optional[dict]:
    """
    Chroma -> (code, distance) ÌõÑÎ≥¥ ÏñªÍ∏∞
    ÏòÅÏñëÍ∞íÏùÄ RDBÏóêÏÑú codeÎ°ú Ï°∞Ìöå
    inverse-distance Í∞ÄÏ§ë ÌèâÍ∑†
    """
    candidates = rag_pick_codes(
        query_text=food_name,
        top_k=top_k,
        hard_threshold=hard_threshold,
        soft_threshold=soft_threshold,
    )
    if not candidates:
        return None

    codes = [c for c, _ in candidates]
    code_to_food = _fetch_foods_by_codes(db, codes)

    # RDBÏóê Ïã§Ï†úÎ°ú Ï°¥Ïû¨ÌïòÎäî Í≤ÉÎßå ÎÇ®ÍπÄ
    valid: List[Tuple[Foods, float]] = []
    for code, dist in candidates:
        f = code_to_food.get(code)
        if f:
            valid.append((f, dist))

    if not valid:
        print("[WARN] RAG returned codes but none found in RDB.")
        return None

    # Top-1 Í∏∞Ï§Ä standardÎ•º Îî∞ÎùºÍ∞ÄÎêò, ÌïÑÏöîÌïòÎ©¥ Ïó¨Í∏∞ÏÑú ÌÜµÏùº Í∑úÏπô Ï†ÅÏö© Í∞ÄÎä•
    best_food = valid[0][0]
    best_dist = valid[0][1]

    # hard thresholdÎäî Ïù¥ÎØ∏ ÌÜµÍ≥ºÌñàÏßÄÎßå, ÌòπÏãú DBÏ™ΩÎßå ÎÇ®Í∏∞Î©¥ÏÑú Ïù¥ÏÉÅÌï¥Ïßà ÏàòÎèÑ ÏûàÏúºÎãà Î°úÍ∑∏
    print(f"[INFO] Weighted RAG from RDB (best_code={best_food.code}, best_distance={best_dist:.4f}, "
          f"used_candidates={len(valid)}/{len(candidates)})")

    weights = [1.0 / (d + eps) for _, d in valid]
    wsum = sum(weights)

    def wavg(attr: str) -> float:
        s = 0.0
        for (food, _), w in zip(valid, weights):
            s += float(getattr(food, attr)) * w
        return s / wsum if wsum > 0 else float(getattr(best_food, attr))

    return {
        "standard": best_food.standard,
        "kcal": wavg("kcal"),
        "carb": wavg("carb"),
        "protein": wavg("protein"),
        "fat": wavg("fat"),
        "sugar": wavg("sugar"),
        "natrium": wavg("natrium"),
        # ÎîîÎ≤ÑÍπÖ/Ï∂îÏ†Å ÌïÑÏöîÌïòÎ©¥ codeÎèÑ Í∞ôÏù¥ ÎÇ¥Î†§ÎèÑ Îê®(ÏßÄÍ∏à response Î™®Îç∏Ïóî ÏóÜÏùå)
        # "code": best_food.code,
    }


# FastAPI
app = FastAPI(title="Foody - Qwen2.5-VL Analyzer API")


@app.on_event("startup")
def on_startup():
    build_chroma_from_db()


@app.post("/api/vlm/food", response_model=FoodResponse)
async def predict_food(
    image: UploadFile = File(...),
    db: Session = Depends(get_db),
):
    # 1) Ïù¥ÎØ∏ÏßÄ Ï≤¥ÌÅ¨
    if not image.content_type or not image.content_type.startswith("image/"):
        raise HTTPException(400, "Ïù¥ÎØ∏ÏßÄ ÌååÏùºÎßå ÏóÖÎ°úÎìú Í∞ÄÎä•Ìï©ÎãàÎã§.")

    # 2) Ïù¥ÎØ∏ÏßÄ Î°úÎìú
    try:
        content = await image.read()
        pil_image = Image.open(io.BytesIO(content)).convert("RGB")
    except Exception:
        raise HTTPException(400, "Ïù¥ÎØ∏ÏßÄÎ•º Ïó¥ Ïàò ÏóÜÏäµÎãàÎã§.")

    # 3) ÏùåÏãù Ïù¥Î¶Ñ Ï∂îÎ°† (VLM)
    print(f"\n{'='*80}")
    food_name = qwen.predict_food_name(pil_image)
    normalized_name = food_name.replace(" ", "")
    print(f"[INFO] üîç VLM Prediction: '{food_name}' (normalized: '{normalized_name}')")

    # 4) RDB exact match (Í≥µÎ∞± Ï†úÍ±∞ ÎπÑÍµê)
    food = (
        db.query(Foods)
        .filter(func.replace(Foods.name, " ", "") == normalized_name)
        .first()
    )

    # 5) ÏòÅÏñë ÏÑ±Î∂Ñ Í≤∞Ï†ï Î°úÏßÅ (Î¶¨Ìå©ÌÑ∞ÎßÅ)
    if food:
        print(f"[INFO] Found exact match in RDB: '{food.name}' (code: {food.code})")
        response_name = food.name
        est = {
            "standard": food.standard,
            "kcal": float(food.kcal),
            "carb": float(food.carb),
            "protein": float(food.protein),
            "fat": float(food.fat),
            "sugar": float(food.sugar),
            "natrium": float(food.natrium),
        }
    else:
        # RAG: ChromaÎäî "code ÌõÑÎ≥¥"Îßå Ï£ºÍ≥†, ÏòÅÏñëÍ∞íÏùÄ RDBÏóêÏÑú Í∞ÄÏ†∏ÏôÄ ÌèâÍ∑†
        print(f"[INFO] No exact match in RDB for '{normalized_name}'")
        print(f"[INFO] Starting RAG search (codes) for '{food_name}'...")

        response_name = food_name
        est = rag_estimate_nutrition_from_rdb(
            db=db,
            food_name=food_name,
            top_k=5,
            hard_threshold=1.0,
            soft_threshold=0.75,
        )

        if est:
            print("[INFO] RAG->RDB nutrition success")
        else:
            print(f"[INFO] RAG failed, using LLM estimation for '{food_name}'")
            est = qwen.estimate_nutrition_llm(food_name)

    # ÏµúÏ¢Ö Í≤∞Í≥º Î°úÍπÖ
    print(f"[INFO] Final Response:")
    print(f"name='{response_name}', standard='{est.get('standard')}'")
    print(f"kcal={round2(est.get('kcal', 0))}, carb={round2(est.get('carb', 0))}g")
    print(f"protein={round2(est.get('protein', 0))}g, fat={round2(est.get('fat', 0))}g")
    print(f"sugar={round2(est.get('sugar', 0))}g, natrium={round2(est.get('natrium', 0))}mg")
    print(f"{'='*80}\n")

    return FoodResponse(
        name=response_name,
        standard=est.get("standard", "100g"),
        kcal=round2(est.get("kcal", 0)),
        carb=round2(est.get("carb", 0)),
        protein=round2(est.get("protein", 0)),
        fat=round2(est.get("fat", 0)),
        sugar=round2(est.get("sugar", 0)),
        natrium=round2(est.get("natrium", 0)),
    )


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=False
    )
